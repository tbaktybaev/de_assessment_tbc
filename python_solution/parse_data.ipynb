{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Contracts and Extracting Features\n",
    "\n",
    "In this notebook, we parse a dataset of contracts stored as JSON strings and extract key features for analysis. \n",
    "\n",
    "### Step 1: Parsing Contracts\n",
    "Contracts are stored as JSON strings, which we convert into structured columns. We extract key details like bank names, contract IDs, and claim dates for easier processing.\n",
    "\n",
    "### Step 2: Data Type Conversion\n",
    "We convert dates and numbers into appropriate formats (datetime and numeric) so that we can calculate and analyze them properly.\n",
    "\n",
    "### Step 3: Feature Extraction\n",
    "We compute useful features:\n",
    "- **`tot_claim_cnt_l180d`**: Number of claims in the last 180 days.\n",
    "- **`disb_bank_loan_wo_tbc`**: Sum of loans from non-excluded banks.\n",
    "- **`day_sinlastloan`**: Days since the last loan.\n",
    "\n",
    "Special cases like missing claims or loans are handled with specific values (`-3` or `-1`).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pandas.core.frame import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_contracts(data: pd.DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Parses the 'contracts' column in the dataset, which contains JSON strings, into structured columns.\n",
    "    \n",
    "    Steps:\n",
    "    - Sorts data by 'id', 'application_date', and 'contracts' to ensure correct ordering.\n",
    "    - Iterates over each row, converts the JSON contract data into a more readable format.\n",
    "    - Each contract in the JSON is broken down into fields like contract_id, bank, summa, etc.\n",
    "    - Handles potential JSON decoding errors.\n",
    "    \n",
    "    Args:\n",
    "    - data (pd.DataFrame): Input dataframe containing contract data, with 'contracts' as a JSON string column.\n",
    "    Returns:\n",
    "    - pd.DataFrame: A dataframe where each contract is expanded into its own row, with structured columns like 'bank', 'summa', etc.\n",
    "    \"\"\"\n",
    "    data = data.sort_values(by=['id', 'application_date', 'contracts']).drop_duplicates(subset=['contracts'], keep='last')\n",
    "\n",
    "    rows = []\n",
    "    for _, row in data.iterrows():\n",
    "        id_value = row['id']\n",
    "        application_date = row['application_date']\n",
    "        contracts_str = row['contracts']\n",
    "\n",
    "        if pd.isna(contracts_str) or contracts_str == '':\n",
    "            continue\n",
    "        else:\n",
    "            try:\n",
    "                contracts = json.loads(contracts_str)\n",
    "                if isinstance(contracts, dict):\n",
    "                    contracts = [contracts]\n",
    "                for contract in contracts:\n",
    "                    rows.append({\n",
    "                        'id': id_value,\n",
    "                        'application_date': application_date,\n",
    "                        'contract_id': contract.get('contract_id'),\n",
    "                        'bank': contract.get('bank'),\n",
    "                        'summa': contract.get('summa'),\n",
    "                        'loan_summa': contract.get('loan_summa'),\n",
    "                        'claim_date': contract.get('claim_date'),\n",
    "                        'claim_id': contract.get('claim_id'),\n",
    "                        'contract_date': contract.get('contract_date')\n",
    "                    })\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def parse_contracts_by_explode(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parses the 'contracts' column in the dataset, which contains JSON strings, into structured columns.\n",
    "\n",
    "    Steps:\n",
    "    - Sorts data by 'id', 'application_date', and 'contracts' to ensure correct ordering.\n",
    "    - Converts the 'contracts' column (stored as JSON strings) into separate rows for each contract.\n",
    "    - Safely handles missing or malformed JSON data.\n",
    "    - Normalizes the JSON structure into a flat table, expanding the contracts into structured columns.\n",
    "\n",
    "    Args:\n",
    "    - data (pd.DataFrame): Input dataframe containing contract data, with 'contracts' as a JSON string column.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: A dataframe where each contract is expanded into its own row, with structured columns like 'bank', 'summa', etc.\n",
    "    \"\"\"\n",
    "    data['id'] = data['id'].astype('Int64')\n",
    "    data['application_date'] = data['application_date'].str.split().str[0]\n",
    "    data['application_date'] = pd.to_datetime(data['application_date'], errors='coerce', utc=True)\n",
    "    data = data.sort_values(by=['id', 'application_date', 'contracts']).drop_duplicates(subset=['contracts'], keep='last')\n",
    "    data['contracts'] = data['contracts'].apply(lambda x: json.loads(x) if pd.notnull(x) and x != '' else [])\n",
    "    data = data.explode('contracts').reset_index(drop=True)\n",
    "    contracts = pd.json_normalize(data['contracts'])\n",
    "    data = pd.concat([data.drop(columns=['contracts']), contracts], axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_types(df: pd.DataFrame, by_explode: bool = False) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Converts important columns to the appropriate data types for further processing.\n",
    "    \n",
    "    Steps:\n",
    "    - Converts 'id' to integer type.\n",
    "    - Converts 'application_date', 'claim_date', and 'contract_date' to datetime format.\n",
    "    - Converts 'summa' and 'loan_summa' to numeric format for calculations.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): Input dataframe with columns needing type conversion.\n",
    "    Returns:\n",
    "    - pd.DataFrame: Dataframe with corrected types for dates and numeric fields.\n",
    "    \"\"\"   \n",
    "    \n",
    "    if not by_explode:\n",
    "        df['id'] = df['id'].astype('Int64')\n",
    "        df['application_date'] = df['application_date'].str.split().str[0]\n",
    "        df['application_date'] = pd.to_datetime(df['application_date'], errors='coerce')\n",
    "    \n",
    "    df['claim_date'] = pd.to_datetime(df['claim_date'], format='%d.%m.%Y', errors='coerce')\n",
    "    df['contract_date'] = pd.to_datetime(df['contract_date'], format='%d.%m.%Y', errors='coerce')\n",
    "\n",
    "    df['summa'] = pd.to_numeric(df['summa'], errors='coerce')\n",
    "    df['loan_summa'] = pd.to_numeric(df['loan_summa'], errors='coerce')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tot_claim_cnt_l180d(df: pd.DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the total number of claims made within the last 180 days from the 'application_date'.\n",
    "    \n",
    "    Steps:\n",
    "    - For each 'id', calculates how many claims fall within the 180-day window.\n",
    "    - If no valid claims are found, returns -3.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): Dataframe with claim dates and application dates.\n",
    "    Returns:\n",
    "    - pd.DataFrame: Dataframe with the count of claims in the last 180 days, grouped by 'id'.\n",
    "    \"\"\"\n",
    "    def claims_in_last_180_days(sub_df):\n",
    "        app_date = sub_df['application_date'].iloc[0]\n",
    "        last_180_days = app_date - timedelta(days=180)\n",
    "        valid_claims = sub_df[(sub_df['claim_date'] >= last_180_days) & (sub_df['claim_date'] <= app_date)]\n",
    "        \n",
    "        return len(valid_claims) if len(valid_claims) > 0 else -3\n",
    "\n",
    "    return df.groupby('id').apply(claims_in_last_180_days, include_groups=False).reset_index(name='tot_claim_cnt_l180d')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_disb_bank_loan_wo_tbc(df: pd.DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Sums the loan amounts from valid banks, excluding certain banks like 'MKO', 'LIZ', etc.\n",
    "    \n",
    "    Steps:\n",
    "    - Excludes loans from specific banks (e.g., 'MKO', 'LIZ', etc.).\n",
    "    - Considers only loans where 'contract_date' is not null (disbursed loans).\n",
    "    - If no valid loans are found, returns -1 or -3 depending on the scenario.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): Dataframe containing loan data, including bank names and loan amounts.\n",
    "    Returns:\n",
    "    - pd.DataFrame: Dataframe with total loan amounts grouped by 'id' from non-excluded banks.\n",
    "    \"\"\"\n",
    "    excluded_banks = ['LIZ', 'LOM', 'MKO', 'SUG', None]\n",
    "\n",
    "    def sum_loans_wo_tbc(sub_df):\n",
    "        valid_loans = sub_df[(sub_df['bank'].notna()) & (~sub_df['bank'].isin(excluded_banks)) & (sub_df['contract_date'].notna())]\n",
    "        total_sum = valid_loans['loan_summa'].sum()\n",
    "        if len(valid_loans) == 0:\n",
    "            return -1\n",
    "        \n",
    "        return total_sum if total_sum > 0 else -3\n",
    "\n",
    "    return df.groupby('id').apply(sum_loans_wo_tbc, include_groups=False).reset_index(name='disb_bank_loan_wo_tbc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_day_sinlastloan(df):\n",
    "    \"\"\"\n",
    "    Calculates how many days have passed since the last valid loan (where 'summa' is not null).\n",
    "    \n",
    "    Steps:\n",
    "    - Finds the last valid loan for each 'id' and calculates the difference in days from the 'application_date'.\n",
    "    - If no valid loans are found, returns -1 or -3 depending on the scenario.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): Dataframe with loan data, including loan amounts and dates.\n",
    "    Returns:\n",
    "    - pd.DataFrame: Dataframe with the number of days since the last loan, grouped by 'id'.\n",
    "    \"\"\"\n",
    "    def days_since_last_loan(sub_df):\n",
    "        valid_loans = sub_df[sub_df['summa'].notna() & (sub_df['summa'] > 0)]\n",
    "        if valid_loans.empty:\n",
    "            return -1\n",
    "        last_loan_date = valid_loans['contract_date'].max()\n",
    "        days_diff = (sub_df['application_date'].iloc[0] - last_loan_date).days\n",
    "        \n",
    "        return days_diff if days_diff > 0 else -3\n",
    "\n",
    "    return df.groupby('id').apply(days_since_last_loan, include_groups=False).reset_index(name='day_sinlastloan')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First option for parsing is to examine the JSON data manually in the contracts and finding all the necessary columns\n",
    "# Th best practise for this type of data is to maintain technical documentation. Thus, we can be sure that we dont miss the important columns\n",
    "# \n",
    "data = pd.read_csv('data.csv')\n",
    "data = parse_contracts(data)\n",
    "data = convert_data_types(data, by_explode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id application_date contract_id bank  summa  loan_summa claim_date  \\\n",
      "2499  2926175       2024-02-13              062    NaN         NaN 2023-02-07   \n",
      "2500  2926175       2024-02-13              MKO    NaN         NaN 2023-12-07   \n",
      "2501  2926175       2024-02-13              MKO    NaN         NaN 2024-01-12   \n",
      "2502  2926175       2024-02-13              063    NaN         NaN 2024-02-13   \n",
      "\n",
      "            claim_id contract_date  \n",
      "2499       PS82-3227           NaT  \n",
      "2500  28857300000000           NaT  \n",
      "2501  35972400000000           NaT  \n",
      "2502       554501463           NaT  \n",
      "id                           Int64\n",
      "application_date    datetime64[ns]\n",
      "contract_id                 object\n",
      "bank                        object\n",
      "summa                      float64\n",
      "loan_summa                 float64\n",
      "claim_date          datetime64[ns]\n",
      "claim_id                    object\n",
      "contract_date       datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "row_with_id = data[data['id'] == 2926175]\n",
    "print(row_with_id)\n",
    "print(row_with_id.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second option for parsing - using built-in method of pandas dataframe method: pd.DataFrame.Explode:\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html\n",
    "\n",
    "df = pd.read_csv(filepath_or_buffer='data.csv')\n",
    "df = parse_contracts_by_explode(data=df)\n",
    "df = convert_data_types(df=data, by_explode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id application_date contract_id bank  summa  loan_summa claim_date  \\\n",
      "2499  2926175       2024-02-13              062    NaN         NaN 2023-02-07   \n",
      "2500  2926175       2024-02-13              MKO    NaN         NaN 2023-12-07   \n",
      "2501  2926175       2024-02-13              MKO    NaN         NaN 2024-01-12   \n",
      "2502  2926175       2024-02-13              063    NaN         NaN 2024-02-13   \n",
      "\n",
      "            claim_id contract_date  \n",
      "2499       PS82-3227           NaT  \n",
      "2500  28857300000000           NaT  \n",
      "2501  35972400000000           NaT  \n",
      "2502       554501463           NaT  \n",
      "id                           Int64\n",
      "application_date    datetime64[ns]\n",
      "contract_id                 object\n",
      "bank                        object\n",
      "summa                      float64\n",
      "loan_summa                 float64\n",
      "claim_date          datetime64[ns]\n",
      "claim_id                    object\n",
      "contract_date       datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "row_with_id = df[df['id'] == 2926175]\n",
    "print(row_with_id)\n",
    "print(row_with_id.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see, there is no difference in pre-final version of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2556 entries, 0 to 2555\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   id                2556 non-null   Int64         \n",
      " 1   application_date  2556 non-null   datetime64[ns]\n",
      " 2   contract_id       2556 non-null   object        \n",
      " 3   bank              2519 non-null   object        \n",
      " 4   summa             550 non-null    float64       \n",
      " 5   loan_summa        505 non-null    float64       \n",
      " 6   claim_date        2556 non-null   datetime64[ns]\n",
      " 7   claim_id          2556 non-null   object        \n",
      " 8   contract_date     550 non-null    datetime64[ns]\n",
      "dtypes: Int64(1), datetime64[ns](3), float64(2), object(3)\n",
      "memory usage: 182.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2556 entries, 0 to 2555\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   id                2556 non-null   Int64         \n",
      " 1   application_date  2556 non-null   datetime64[ns]\n",
      " 2   contract_id       2556 non-null   object        \n",
      " 3   bank              2519 non-null   object        \n",
      " 4   summa             550 non-null    float64       \n",
      " 5   loan_summa        505 non-null    float64       \n",
      " 6   claim_date        2556 non-null   datetime64[ns]\n",
      " 7   claim_id          2556 non-null   object        \n",
      " 8   contract_date     550 non-null    datetime64[ns]\n",
      "dtypes: Int64(1), datetime64[ns](3), float64(2), object(3)\n",
      "memory usage: 182.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature calculation completed and saved to contract_features.csv\n"
     ]
    }
   ],
   "source": [
    "application_dates = data[['id', 'application_date']].drop_duplicates()\n",
    "\n",
    "tot_claim_cnt_l180d = calculate_tot_claim_cnt_l180d(data)\n",
    "disb_bank_loan_wo_tbc = calculate_disb_bank_loan_wo_tbc(data)\n",
    "day_sinlastloan = calculate_day_sinlastloan(data)\n",
    "\n",
    "features_df = tot_claim_cnt_l180d.merge(disb_bank_loan_wo_tbc, on='id').merge(day_sinlastloan, on='id').merge(application_dates, on='id')\n",
    "\n",
    "columns_order = ['id', 'application_date', 'tot_claim_cnt_l180d', 'disb_bank_loan_wo_tbc', 'day_sinlastloan']\n",
    "features_df = features_df[columns_order]\n",
    "\n",
    "features_df.to_csv('contract_features.csv', index=False)\n",
    "\n",
    "print(\"Feature calculation completed and saved to contract_features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данную задачу можно было реализовать несколькими способами:\n",
    "1. Использовать DAG в Airflow для автоматической обработки.\n",
    "2. Создание Python приложения\n",
    "3. Простой подход с помощью Jupyter Notebook для визуального анализа и тестирования.\n",
    "\n",
    "Каждый вариант имеет свои плюсы в зависимости от цели и объема данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
